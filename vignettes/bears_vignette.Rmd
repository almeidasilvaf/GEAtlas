---
title: "bears: building expression atlases in R"
author: 
  - name: Fabricio Almeida-Silva
    affiliation: Universidade Estadual do Norte Fluminense Darcy Ribeiro, RJ, Brazil
  - name: Thiago Motta Venancio
    affiliation: Universidade Estadual do Norte Fluminense Darcy Ribeiro, RJ, Brazil
output: 
  BiocStyle::html_document:
    self_contained: yes
    toc: true
    toc_depth: 3
    number_sections: yes
bibliography: bibliography.bib
date: "`r Sys.Date()`"
vignette: >
  %\VignetteIndexEntry{Building expression atlases in R}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}  
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
    collapse = TRUE,
    comment = "#>",
    eval = FALSE,
    crop = NULL ## Related to https://stat.ethz.ch/pipermail/bioc-devel/2020-April/016656.html
)
```

# Introduction

In the past decades, there has been an exponential accumulation of RNA-seq
data in public repositories. This steep increase paved the way to the creation
of gene expression atlases, which consist in comprehensive collections of
expression data from public databases, analyzed with a single pipeline for
consistency and cross-project comparison. 
`r BiocStyle::Githubpkg("almeidasilvaf/bears")` 
is a package that allows you to create your own gene expression atlas 
for a given species using public data. The package features:

- Data download from NCBI's Sequence Read Archive (SRA) or the European
Nucleotide Archive (ENA).
- Sequence quality check and trimming of low-quality sequences.
- Removal of rRNA, which are typical problems of libraries that were prepared
with the rRNA depletion protocol.
- Read mapping against a reference genome.
- Transcript assembly.
- Quantification of gene- and transcript-level transcript abundance with 
**alignment-based** and **alignment-free** methods.

# Installation and setup

To install `r BiocStyle::Githubpkg("almeidasilvaf/bears")`, 
use the following code:

```{r install, eval = FALSE}
remotes::install_github("almeidasilvaf/bears")
```

Then, create a standard directory structure with `create_dir_structure()` to
store your results. This is optional, 
but **it will make your life much easier.** The output is a list of paths to
common directories that you will need to specify in several functions of this
package.

```{r setup_dir}
library(bears)

# Create directory structure using a temporary directory as root
ds <- create_dir_structure(rootdir = tempdir())

# Look at the output
ds
```

To run the full pipeline implemented in 
`r BiocStyle::Githubpkg("almeidasilvaf/bears")`, you will need to have some
external software tools installed in your machine. 
The names of the tools are listed below. In the **Functions** column, you
can see the names of functions in
`r BiocStyle::Githubpkg("almeidasilvaf/bears")` that require each tool.

```{r external_software, echo = FALSE, fig.align='center', eval = TRUE}
software_df <- data.frame(
    Software = c("FastQC", "kallisto", "MultiQC", "RSeQC",
                 "salmon", "SortMeRNA", "STAR", "StringTie",
                 "Subread", "TACO", "Trimmomatic"),
    Version = c(">=0.11.9", ">=0.11.9", ">=1.12", ">=4.0.0",
                ">=1.8.0", ">=4.3.4", ">=2.7.10a", ">=2.2.1",
                ">=2.0.1", ">=0.7.3", ">=0.39")
)
software_df$Functions <- c(
    "run_fastqc()", "kallisto_index(), kallisto_quantify()",
    "multiqc()", "infer_strandedness()", 
    "salmon_index(), salmon_quantify()", "remove_rrna()",
    "star_genome_index(), star_align()", 
    "stringtie_assemble(), stringtie_quantify()",
    "fcount()", "taco_merge()", "trim_reads()"
)
knitr::kable(software_df)
```

To make your life easier, we have created .yml files with Conda environments
containing each of these external tools. In this vignette, we will manage
external dependencies with the Bioconductor package
`r BiocStyle::Biocpkg("Herper")`. We will 
use `BiocStyle::Biocpkg("Herper")` to:

1. **Install miniconda**. For this vignette, we will install miniconda in a 
temporary directory, which means that miniconda will be automatically deleted
when the R session closes. 

> "But I already have miniconda installed!"
    
Great! In that case, just create an object (e.g., `my_miniconda`) with the
path to your miniconda installation.
    
> "Can I have a permanent installation of miniconda instead of installing 
it in a temporary directory?"
    
For sure. You can specify a path to a different directory in your object
containing the miniconda installation path. For instance, you can try 
`my_miniconda <- "~/Documents/miniconda"`.


2. **Create a different environment for each external tool**. To avoid 
conflicts, it is important to keep each tool in its own environment. Here,
each tool will be installed in an environment named `<tool-name-lowercase>_env` 
(e.g., `star_env`, `rseqc_env`, `fastqc_env`).


3. **Run R functions that require external tools**. We will run functions that
call external tools inside a call to `Herper::withCondaEnv()`. This 
`r BiocStyle::Biocpkg("Herper")` function allows us to run an R function inside
a particular Conda environment, which we need to specify.

Below is the code to install miniconda and create tool-specific 
environments from the .yml files.

```{r install_conda_envs}
library(Herper)

# Path to directory where miniconda will be installed
my_miniconda <- file.path(tempdir(), "miniconda")

# Path to .yml files to create environments
envs <- list.files(
    system.file("extdata", package = "bears"), pattern = ".yml",
    full.names = TRUE
)

# Install miniconda in `my_miniconda` and create envs
create_envs <- sapply(envs, function(x) {
    import_CondaEnv(x, pathToMiniConda = my_miniconda)
})
```

Let's see the environments and their names.

```{r check_envs}
list_CondaEnv(pathToMiniConda = my_miniconda)
```

```{r test_installation, eval = TRUE}
library(bears)

# Sys.setenv(
#     PATH = paste0(
#         Sys.getenv("PATH"), 
#         "/opt/FastQC/fastqc",
#         "/opt/STAR-2.7.9a/bin/Linux_x86_64_static/",
#         "/opt/bin/",
#         "/opt/salmon-1.5.2_linux_x86_64/bin/",
#         "/opt/kallisto/",
#         "/opt/subread-2.0.3-Linux-x86_64/bin/",
#         "/opt/stringtie-2.1.7.Linux_x86_64/",
#         "/opt/taco-v0.7.3.Linux_x86_64/",
#         sep = ":"
#     )
# )

# Test software installation
fastqc_is_installed()
multiqc_is_installed()
star_is_installed()
trimmomatic_is_installed()
sortmerna_is_installed()
rseqc_is_installed()
salmon_is_installed()
kallisto_is_installed()
subread_is_installed()
stringtie_is_installed()
taco_is_installed()
```


# Retrieving sample metadata

First of all, you need to choose which samples you want to download and create
a **metadata data frame** for your samples. To create this data frame, you will
pass a search term to the function `create_sample_info()`. The search term
has the same syntax of the SRA search term syntax. For example, you can search
by:

- BioSample accession - **SAMN08903403[BSPL]**
- BioProject accession - **PRJNA229998[GPRJ]**
- All RNA-seq samples for an organism - **Glycine max[ORGN] AND RNA-seq[STRA]**
- And so on...

Let's create a metadata data frame for a human RNA-seq sample that is included
in the `r BiocStyle::Biocpkg("airway")` Bioconductor package.

```{r sample_metadata}
# Create metadata data frame
term <- "SAMN02422669[BSPL]"
metadata <- create_sample_info(term)
metadata
```

# Downloading FASTQ files

To download the .fastq files with the reads, you have 2 options:

- Using `download_from_ena()` (recommended), which downloads reads from ENA. 
This function does not require any external dependency and is much
faster than using *SRAToolkit*.

- Using `download_fastq()`, which relies on *SRAToolkit* to 
download reads from NCBI's SRA. You need to have *SRAToolkit* installed 
in your machine to run this function. This function only exists for
historical reasons (i.e., some people use *SRAToolkit* for a long time
and are used to it). You should try to avoid *SRAToolkit*, because it is slow
and not so easy to set up.


As input, you only need to give the metadata data frame and the path
the output directory where .fastq files will be stored. For example:

```{r download_from_ena, eval = FALSE}
# Download sample to temporary directory
options(timeout = 6000)
download <- download_from_ena(metadata, fastqdir = ds$fastqdir)

# For running time issues, copy example FASTQ files to ds$fastqdir
f <- list.files(system.file("extdata", package = "bears"), 
                pattern = ".fastq.gz", full.names = TRUE)
f
copy <- file.copy(f, ds$fastqdir)
```

It's important to check your downloaded FASTQ files for download
issues that might happen when the internet connection is not stable.
The function `check_downloads()` checks your downloaded FASTQ files
to look for one of the three issues below:

- **Issue 1:** File was not downloaded.
- **Issue 2:** For paired-end reads, only forward or reverse file 
was downloaded.
- **Issue 3:** Less reads than the expected.

To look for **issue 3**, `check_downloads()` needs to know the expected number
of reads for each run accession. This number is available in SRA, and it 
can be obtained with the function `get_read_count()`. For example:

```{r get_read_count}
# Get a data frame of run accessions and their read counts
read_count <- get_read_count(metadata, run_accession = metadata$Run)
read_count
```

As you can see, this function returns a data frame with run accessions and
their number of reads as reported by SRA. With this information, you can
run `check_downloads()` to look for download problems.

```{r check_downloads}
# Check downloaded files for issues
check_downloads(metadata, fastqdir = ds$fastqdir, read_count)
```

Here, as you can see, the function reported an issue 3 
(*less reads than the expected*). That is because we are using the example
FASTQ files that come with this package, which have been filtered for 
package size issues.


# Sequence quality check

After downloading the .fastq files, you need to perform some quality checks
to check if you don't have poor-quality base calling or sequence adapters. 
These can be done with the functions `run_fastqc()` and `multiqc()`, which
will run *FastQC* [@andrews2019] and summarize *FastQC* results 
with MultiQC [@ewels2016multiqc].

**NOTE:** Both `run_fastqc()` and `multiqc()` require external tools. Throughout
this vignette, we will execute functions that call external tools inside 
`Herper::with_CondaEnv()` calls.

Here, for the purpose of demonstration,
we will use example FASTQ files stored in /extdata. 

```{r run_fastqc}
# Path to MultiQC output
multiqc_out <- file.path(tempdir(), "multiqc")

# Run FastQC for all .fastq files in fastqdir
with_CondaEnv(
    "fastqc_env",
    fastqc <- run_fastqc(
        metadata, 
        fastqdir = ds$fastqdir,
        fastqcdir = ds$fastqcdir
    ),
    pathToMiniConda = my_miniconda
)
fastqc # check status


# Summarize FastQC results with MultiQC
with_CondaEnv(
    "multiqc_env",
    fastqc_table <- multiqc(ds$fastqcdir, outdir = multiqc_out),
    pathToMiniConda = my_miniconda
)

fastqc_table # check results
```

# Read filtering

Read filtering consists in 2 steps:

1. `trim_reads()` - trim adapters and low-quality bases. This function
runs *Trimmomatic* [@bolger2014trimmomatic] and saves filtered .fastq files 
in a directory named `filtdir`.
2. `remove_rrna()` - remove rRNA (if any) from .fastq files. rRNA
removal relies on the *SortMeRNA* [@kopylova2012sortmerna] program.

Here, as you can see in `fastqc_table`, the files passed the "Per
Base Sequence Quality" check, so we don't need to trim anything. For this
particular case, we can skip `trim_reads()` and only run `remove_rrna()`.
As the rRNA database, we will create a directory and store an example rRNA file
in it.

In real-world applications, your rRNA database directory should contain
all FASTA files distributed in the 
[SortMeRNA GitHub repo](https://github.com/biocore/sortmerna/tree/master/data/rRNA_databases).
However, if you think some of these files (e.g., 5s and 5.8s rRNA) are not a 
concern in your data set, you don't need to include them in the database.

```{r remove_rrna}
# Create a directory to store the rRNA db
rrna_db_dir <- file.path(tempdir(), "rrna")
dir.create(rrna_db_dir)

# Copy the example 16S rRNA file to the db directory.
rrna_file <- system.file("extdata", "bac_16s_subset.fa", package="bears")
copy <- file.copy(from = rrna_file, to = rrna_db_dir)

# Run SortMeRNA from the Conda env
with_CondaEnv(
    "sortmerna_env",
    rrna_removal <- remove_rrna(
        metadata,
        fastqdir = ds$fastqdir,
        filtdir = ds$filtdir,
        rrna_db_dir = rrna_db_dir
    ),
    pathToMiniConda = my_miniconda
)
rrna_removal # check status
```

Now that we have performed all quality checks, we're good to go. [^1]

[^1]: **NOTE:** After running `remove_rrna()`, filtered .fastq files are stored
in the directory specified in `filtdir`. If you think this step is not 
necessary for your data set and want to skip it, the `filtdir` directory will
remain empty, so all your .fastq files will be stored in `fastqdir`. Keep this
in mind when specifying the path to .fastq files in the following steps of
the pipeline. If that is your case, you can make `fastqdir` and `filtdir` the 
same with `ds$filtdir <- ds$fastqdir`.


# Quantification of transcript abundance

Quantification of transcript abundance can be done in two ways:

1. **Alignment-based approaches**, which involves mapping reads to a reference
genome using *STAR* [@dobin2013star] and quantifying the expression based on 
uniquely mapped reads with 
*featureCounts* [@liao2014featurecounts] (in raw counts) and 
*StringTie* [@pertea2015stringtie] (in TPM).
2. **Alignment-free approaches**, which involves pseudo-aligning or 
quasi-mapping reads to a reference transcriptome with 
*kallisto* [@bray2016near] or *salmon* [@patro2017salmon],
respectively.

Below, we will describe how to perform both approaches.

## Alignment-based quantification

To start with, we will need to map the reads to a reference genome. 
In `r BiocStyle::Githubpkg("almeidasilvaf/bears")`, reads are mapped 
to the reference genome with the software tool *STAR*.

### Read mapping

Here, for the purpose of demonstration, we will map reads to a subset of the
human genome. The FASTA and GTF files corresponding to the subset of the genome
are available in /extdata.

Before mapping reads, we need to create a genome index. This can be done 
with `star_genome_index()`.

```{r star_genome_index}
# Get paths to genome subset
genome_path <- system.file("extdata", "Homo_sapiens.GRCh37.75_subset.fa", 
                            package="bears")
gff_path <- system.file("extdata", "Homo_sapiens.GRCh37.75_subset.gtf", 
                         package="bears")

# Create genome index
with_CondaEnv(
    "star_env",
    genome_idx <- star_genome_index(
        genome_path = genome_path, 
        gff_path = gff_path, 
        mappingdir = ds$mappingdir
    ),
    pathToMiniConda = my_miniconda
)
genome_idx # check status
```

Now that we have the genome index, we can map reads to it.

```{r star_align}
# Map reads to the genome
with_CondaEnv(
    "star_env",
    read_mapping <- star_align(
        metadata, 
        filtdir = ds$filtdir,
        fastqc_table = fastqc_table,
        mappingdir = ds$mappingdir,
        gff_path = gff_path
    ),
    pathToMiniConda = my_miniconda
)
```

Finally, let's get read mapping statistics.

```{r mapping_stats}
# Obtaining read mapping statistics
with_CondaEnv(
    "multiqc_env",
    star_stats <- multiqc(
        dir = ds$mappingdir, 
        outdir = tempdir(),
        runon = "star"
    ),
    pathToMiniConda = my_miniconda
)
star_stats
```

Now, let's check if samples passed the minimum quality criteria. Here, samples
are excluded if:

1. \>=50% of the reads fail to map or;
2. \>=40% of the reads fail to uniquely map.

The function `mapping_pass()` takes the metadata data frame and returns the
same data frame, but only with the samples that passed the minimum criteria.

```{r mapping_pass}
# Check if samples passed the filtering criterion
align_passed <- mapping_pass(star_stats, metadata)
align_passed # inspect data

# Compare to the original data set
nrow(metadata)
nrow(align_passed)
```

As you can see, the sample we used for read mapping passed the minimum
quality criteria. Good, huh? We can now proceed to the next step.

### Inferring library strandedness

Before quantification, we need to infer library strandedness with the 
*RSeQC* [@wang2012rseqc] tool. The function `infer_strandedness()` runs 
*RSeQC* and returns the metadata data frame with an additional column 
named **Orientation** containing library strandedness information. 

This function requires the annotation in BED format, not GTF/GFF. To convert
from GTF/GFF to BED, use the function `gff2bed()`.

```{r infer_strandedness}
# Convert GFF to BED
bedpath <- gff2bed(gff_path)
bedpath # check status

# Infer strandedness
with_CondaEnv(
    "rseqc_env",
    new_metadata <- infer_strandedness(
        mapping_passed = align_passed,
        bedpath = bedpath$bed_path,
        mappingdir = ds$mappingdir
    ),
    pathToMiniConda = my_miniconda
)

new_metadata
```

As you can see, there is a new **Orientation** column with strandedness info
for this sample.

### Using featureCounts

Now that we have the BAM files from *STAR* and information on library 
strandedness for each sample, we quantify the expression with *featureCounts*. 
This tool quantifies gene expression measured in raw read counts per gene. 

To quantify gene expression with *featureCounts*, use the function `fcount()`.
This function runs *featureCounts* and returns a gene expression matrix with
genes in rows and samples in columns.

```{r featureCounts}
# Get gene expression in raw read counts
with_CondaEnv(
    "subread_env",
    fcounts_quant <- fcount(
        new_metadata, 
        mappingdir = ds$mappingdir,
        gff_path = gff_path,
        fcountsdir = ds$fcountsdir
    ),
    pathToMiniConda = my_miniconda
)

# Explore gene expression matrix
fcounts_quant 
```

Whenever you are working with gene expression data, we recommend storing your
data as `SummarizedExperiment` objects, so you have the expression matrix and 
sample metadata in a single object. If you are not familiar with 
`SummarizedExperiment` objects, take a look at the documentation of the 
`r BiocStyle::Biocpkg("SummarizedExperiment")` package. 

To get a `SummarizedExperiment` object from *featureCounts*, use the function
`featureCounts2se()`.

```{r fcounts2se}
# Create a SummarizedExperiment object from featureCounts output
fcountsse <- featureCounts2se(
    new_metadata, fc_output = fcounts_quant
)

# Take a look at the SummarizedExperiment object
fcountsse

# Exploring sample metadata
SummarizedExperiment::colData(fcountsse)

# Exploring gene expression matrix
SummarizedExperiment::assay(fcountsse)
```


### Using StringTie

*StringTie* quantifies transcript-level and gene-level transcript 
abundances in normalized values (transcripts per million, TPM).

To obtain gene expression levels in TPM with *StringTie*, use the function
`stringtie_quantify()`.

```{r stringtie_quantify}
# Quantify expression in TPM with StringTie
with_CondaEnv(
    "stringtie_env",
    stringtie_quant <- stringtie_quantify(
        new_metadata,
        fastqc_table = fastqc_table,
        mappingdir = ds$mappingdir,
        gff_path = gff_path,
        stringtiedir = ds$stringtiedir
    ),
    pathToMiniConda = my_miniconda
)

stringtie_quant # Check status
```

Now, let's read the output from *StringTie* as a `SummarizedExperiment` object
with the function `stringtie2se`. You can choose if you want the expression
at the gene level, at the transcript level, or both. Here, let's get the
gene-level expression. For that, you will need to give a 2-column data frame
with transcript IDs and their corresponding genes.

```{r stringtie2se}
# Load transcript-to-gene correspondence
data(tx2gene)
head(tx2gene)

# Read StringTie output as a SummarizedExperiment object
stringtiese <- stringtie2se(
    new_metadata,
    stringtiedir = ds$stringtiedir,
    level = "gene",
    tx2gene = tx2gene
)

# Exploring the SummarizedExperiment object
stringtiese

# Looking at gene expression matrix
SummarizedExperiment::assay(stringtiese, "gene_TPM")
```

### Bonus: Transcript assembly and merging

Besides quantifying transcript abundance, *StringTie* can also be used to
assemble transcripts for each BioSample. Assembled transcripts for each
BioSample are represented as .gtf files.

However, if you want a single .gtf file with the assembled transcripts for all
BioSamples you are studying, you can merge the individual .gtf files from
*StringTie* with the software tool *TACO* [@niknafs2017taco]. Below, we will 
demonstrate how that can be achieved.

```{r transcript_assembly}
# Transcript assembly with StringTie
with_CondaEnv(
    "stringtie_env",
    assembled_transcripts <- stringtie_assemble(
        new_metadata,
        fastqc_table = fastqc_table,
        mappingdir = ds$mappingdir,
        gff_path = gff_path,
        stringtiedir = ds$stringtiedir
    ),
    pathToMiniConda = my_miniconda
)

assembled_transcripts # check status
```

In this vignette, we have a single BioSample. However, in real-life scenarios,
you would have several samples. To merge the .gtf files for each sample in
a single .gtf file, use the function `taco_merge()`.

```{r taco_merge}
# Merge assembled transcripts with TACO
with_CondaEnv(
    "taco_env",
    merged_transcripts <- taco_merge(
        new_metadata,
        stringtiedir = ds$stringtiedir
    ),
    pathToMiniConda = my_miniconda
)

merged_transcripts # check status
```

The merged transcript assembly will be stored in a file named
**final_assembly.gtf** in the subdirectory 
*assembly/merged_assembly*, inside `stringtiedir`. To get the path
to the .gtf file, use:

```{r merged_transcript_path}
# Get path to merged transcript assembly
final_assembly <- file.path(
    ds$stringtiedir, "assembly", "merged_assembly", "final_assembly.gtf"
)

final_assembly
```



> But why would I want to assemble transcripts and merge them if I already have
a .gtf file the transcript annotations?

That's a great question! This is a way to identify novel transcripts that are
not present in your reference .gtf file. Some transcripts can be missing in
the reference annotation (.gtf file) mainly because:
i. genome assembly does not have a good quality, so these transcripts
could not be predicted.
ii. false-positives from the transcript annotation software tool that was used.


If you want to have a more comprehensive transcript abundance quantification,
you can assemble transcripts for each sample, merge them, and input the 
output file **final_assembly.gtf** to the quantification functions. This way,
instead of using the reference transcript annotation, you will use your own
transcript annotation, which may contain novel transcripts.


## Alignment-free quantification

To quantify the expression without mapping reads to the genome, you have two 
options:

- **Using kallisto**, which quantifies transcript abundance based on 
pseudoalignments.
- **Using salmon**, which quantifies transcript abundance based on
quasi-mapping.

For both kallisto and salmon, you will need to have a 
**reference transcriptome**, not a reference genome. This is a FASTA file
containing the sequences of all annotated transcripts in your genome. You can
easily create this file with the function `extractTranscriptSeqs()` from the
`r BiocStyle::Biocpkg("GenomicFeatures")` package.

### Using salmon

First of all, we will need to index the reference transcriptome with the
function `salmon_index()`.

```{r salmon_index}
# Path to reference transcriptome
transcriptome_path <- system.file(
    "extdata", "Homo_sapiens.GRCh37.75_subset_transcripts.fa.gz",
    package = "bears"
)

# Index the transcriptome
with_CondaEnv(
    "salmon_env",
    idx_salmon <- salmon_index(
        salmonindex = ds$salmonindex,
        transcriptome_path = transcriptome_path
    ),
    pathToMiniConda = my_miniconda
)

idx_salmon # check status
```

Now, we can quantify transcript abundance with `salmon_quantify()`.

```{r salmon_quantify}
# Quantify transcript abundance
with_CondaEnv(
    "salmon_env",
    quant_salmon <- salmon_quantify(
        new_metadata,
        filtdir = ds$filtdir,
        salmonindex = ds$salmonindex,
        salmondir = ds$salmondir
    ),
    pathToMiniConda = my_miniconda
)

quant_salmon # check status
```

After running `salmon_quantify()`, salmon output in .sf format will be stored 
in the directory specified in `salmondir`. 


To read salmon output as a `SummarizedExperiment` object, 
use the function `salmon2se()`. You can choose if you want the expression
at the gene level, at the transcript level, or both. Here, let's get the
gene-level expression. For that, you will need to give a 2-column data frame
with transcript IDs and their corresponding genes.

```{r salmon2se}
# Load transcript-to-gene data frame
data(tx2gene)
head(tx2gene)

# Read salmon output as a SummarizedExperiment object
salmonse <- salmon2se(
    new_metadata, 
    level = "gene", 
    salmondir = ds$salmondir,
    tx2gene
)

# Exploring the output
salmonse

# Get gene expression matrix in TPM
SummarizedExperiment::assay(salmonse, "gene_TPM")

# Get gene expression matrix as raw counts
SummarizedExperiment::assay(salmonse, "gene_counts")
```

### Using kallisto

Like we do in salmon, we will start by indexing the transcriptome.

```{r kallisto_index}
# Index the transcriptome
with_CondaEnv(
    "kallisto_env",
    idx_kallisto <- kallisto_index(
        kallistoindex = ds$kallistoindex,
        transcriptome_path = transcriptome_path
    ),
    pathToMiniConda = my_miniconda
)

idx_kallisto # check status
```

Now, we can quantify the transcript abundance.

```{r kallisto_quant}
# Quantify transcript abundance
with_CondaEnv(
    "kallisto_env",
    quant_kallisto <- kallisto_quantify(
        new_metadata,
        fastqc_table, 
        filtdir = ds$filtdir,
        kallistoindex = ds$kallistoindex,
        kallistodir = ds$kallistodir
    ),
    pathToMiniConda = my_miniconda
)

quant_kallisto # check status
```

To read kallisto output in a SummarizedExperiment object, use
the function `kallisto2se()`. Again, you will have specify if you want the 
expressiona at the gene level, transcript level, or both. Let's get the gene
expression here.

```{r}
# Read kallisto output to SummarizedExperiment object
kallistose <- kallisto2se(
    new_metadata, 
    level = "gene", 
    kallistodir = ds$kallistodir,
    tx2gene
)

# Exploring the output
kallistose

# Get gene expression matrix in TPM
SummarizedExperiment::assay(kallistose, "gene_TPM")

# Get gene expression matrix as raw counts
SummarizedExperiment::assay(kallistose, "gene_counts")
```

# Closing remarks

If you are using `r BiocStyle::Githubpkg("almeidasilvaf/bears")`, there are two
things you must keep in mind. First, this package was designed to be as 
complete as possible, which means you don't need to run the complete pipeline
for your own project. For instance, if you just want gene expression values
in TPM for a particular BioProject or set of BioProjects, you can simply go
through the *salmon* path of the pipeline, skipping the read mapping section.
Likewise, if you are using `r BiocStyle::Githubpkg("almeidasilvaf/bears")`
for your own data set and you have already cleaned the reads, you can skip the
sequence quality checks and read filtering sections. The second thing to
consider is that `r BiocStyle::Githubpkg("almeidasilvaf/bears")` is a work
in progress. Bioinformatics is a fast-evolving field, and new (and better)
methods to address a particular question are developed continuously. Hence,
we aim to keep `r BiocStyle::Githubpkg("almeidasilvaf/bears")` up to date
with state-of-the-art methods.


# Session information {.unnumbered}

This document was created under the following sections:

```{r session_info}
sessioninfo::session_info()
```

# References {.unnumbered}
